# -*- coding: utf-8 -*-
"""ML_linear_regression.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1vdko9eS9R3NSj1c4gma8xhCriSFybn3q

import matplotlib.pyplot as plt
import numpy as np
import pandas as pd

x_and_y = [(0, 1), (1, 3), (2, 2), (3, 5), (4, 7), (5, 8), (6, 8), (7,9), (8, 10), (9, 12)]
data = pd.DataFrame(x_and_y, columns=['x', 'y'])
data

x = data['x']
y = data['y']
plt.scatter(x, y)

b = 0
w = 1

y_predicted = b + w*x 


error = y - y_predicted 
print(f" initial quadratic error mean: {np.mean(error**2)}")

L2 = 0.5*np.mean(error**2)

print(f"\L2 value: {L2}\n")

# estiamted guesses for w
w_guess = np.linspace(0.75, 2.0, num=20)
L2_list = []
for w in w_guess:
  y_predicted = b +w*x
  
  error = y - y_predicted
  L2 = 0.5*np.mean(error**2)
  L2_list.append(L2)

print(L2_list)

figure , (ax1, ax2) = plt.subplots(1, 2)

#plotting dataset, initial linear regression
ax1.scatter(x, y)
ax1.plot(x, y_predicted)
ax2.plot(w_guess, L2_list)

# improving regression line with initial guesses
b = 0
w = 1.5

#set hyperparameters:

epochs = 10000
learning_rate = 0.01 # reduce if necessary to 0.005


for epoch in range(epochs):
  y_predicted = b + w*x 
  error = y - y_predicted
  L2 = 0.5*np.mean(error**2)
  
  gradient_b = -np.mean(error)
  b = b - learning_rate*gradient_b

  gradient_w = -np.mean(error*x)
  w = w - learning_rate*gradient_w

  if epoch%(epochs/10) == 0:
    print(f"Epoch: {epoch} e Loss Funtion: {L2}") 

print("final w, b: \n ")
print(w,b)

# final plots after corrections:
plt.scatter(x, y)
plt.plot(x, y_predicted)
print(y_predicted)